#!/usr/bin/env python3
"""
Performance Graphs & PDF Report Generator (Standalone)
Generates consolidated performance analysis PDF with multiple graphs

Features:
- Latency percentiles comparison
- Latency vs throughput trade-off analysis
- Power vs performance correlation
- GPU memory usage timeline
- Relative performance comparison
- Distribution analysis

Usage:
    python scripts/generate_performance_graphs.py [--output report.pdf]

Disclaimer: This code is for educational purposes only. Data is synthetic.
Author: Hamna
Target: NVIDIA Performance Engineering
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
from datetime import datetime
import argparse
import os
import csv


def create_title_page(title="Performance Analysis Report"):
    """Create a title page."""
    fig = plt.figure(figsize=(8.5, 11))
    ax = fig.add_subplot(111)
    ax.axis("off")

    # Title
    ax.text(0.5, 0.85, title, ha="center", va="top", fontsize=28, fontweight="bold", transform=ax.transAxes)

    # Subtitle
    ax.text(
        0.5,
        0.78,
        "Performance Analysis Report",
        ha="center",
        va="top",
        fontsize=16,
        color="gray",
        transform=ax.transAxes,
    )

    # Details
    y_pos = 0.65
    ax.text(0.5, y_pos, "Report Details", ha="center", va="top", fontsize=14, fontweight="bold", transform=ax.transAxes)

    details = {
        "System": "NVIDIA Drive Orin AGX",
        "Analysis Type": "Comprehensive Performance Report",
        "Generated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }

    y_pos -= 0.08
    for key, value in details.items():
        ax.text(0.25, y_pos, f"{key}:", ha="right", va="top", fontsize=11, fontweight="bold", transform=ax.transAxes)
        ax.text(0.27, y_pos, str(value), ha="left", va="top", fontsize=11, transform=ax.transAxes)
        y_pos -= 0.05

    # Footer
    ax.text(
        0.5,
        0.05,
        f"Generated by AutoPerfPy",
        ha="center",
        va="bottom",
        fontsize=9,
        style="italic",
        color="gray",
        transform=ax.transAxes,
    )

    return fig


# Global container for optional input-derived data
INPUT = None


def load_input_data(csv_path):
    """Load a simple CSV with expected columns and build helper structures.
    Expected minimal columns: workload,p50,p95,p99,batch,latency,throughput,memory,power,performance,config
    """
    rows = []
    try:
        with open(csv_path, newline="") as fh:
            reader = csv.DictReader(fh)
            for r in reader:
                rows.append(r)
    except Exception:
        return None

    if not rows:
        return None

    # Latency percentiles by workload
    latency_data = {}
    for r in rows:
        w = r.get("workload") or r.get("Workload")
        if not w:
            continue
        try:
            p50 = float(r.get("p50") or r.get("P50") or r.get("latency") or 0)
            p95 = float(r.get("p95") or r.get("P95") or p50)
            p99 = float(r.get("p99") or r.get("P99") or p95)
        except Exception:
            p50 = p95 = p99 = 0.0
        latency_data[w] = {"P50": p50, "P95": p95, "P99": p99}

    # Tradeoff: aggregate by batch if present
    tradeoff_map = {}
    for r in rows:
        b = r.get("batch")
        if b is None:
            continue
        try:
            batch = int(b)
            lat = float(r.get("latency") or r.get("lat") or 0)
            thr = float(r.get("throughput") or 0)
        except Exception:
            continue
        tradeoff_map.setdefault(batch, []).append((lat, thr))

    if tradeoff_map:
        batches = sorted(tradeoff_map.keys())
        batch_lat = [sum(x[0] for x in tradeoff_map[b]) / len(tradeoff_map[b]) for b in batches]
        batch_thr = [sum(x[1] for x in tradeoff_map[b]) / len(tradeoff_map[b]) for b in batches]
        tradeoff = (batches, batch_lat, batch_thr)
    else:
        tradeoff = None

    # Power vs performance
    power_list = []
    perf_list = []
    wl_list = []
    for r in rows:
        try:
            p = float(r.get("power") or 0)
            perf = float(r.get("performance") or r.get("fps") or 0)
            w = r.get("workload") or r.get("Workload") or r.get("config")
        except Exception:
            continue
        wl_list.append(w)
        power_list.append(p)
        perf_list.append(perf)

    power_perf = None
    if wl_list:
        power_perf = (wl_list, power_list, perf_list)

    # Relative data grouped by config
    rel = {}
    for r in rows:
        cfg = r.get("config") or r.get("Config") or r.get("workload")
        try:
            lat = float(r.get("latency") or r.get("p50") or 0)
            thr = float(r.get("throughput") or 0)
            p = float(r.get("power") or 0)
        except Exception:
            lat = thr = p = 0
        rel.setdefault(cfg, {})
        rel[cfg].update({"latency": lat, "throughput": thr, "power": p})

    return {"rows": rows, "latency_data": latency_data, "tradeoff": tradeoff, "power_perf": power_perf, "relative": rel}


def plot_latency_percentiles():
    """Create latency percentiles comparison."""
    fig, ax = plt.subplots(figsize=(12, 6))

    # Use input data if provided, otherwise fall back to synthetic values
    global INPUT
    if INPUT and "latency_data" in INPUT:
        latency_data = INPUT["latency_data"]
        workloads = list(latency_data.keys())
    else:
        workloads = ["ResNet50", "YOLO V8", "Segformer", "EfficientDet"]
        latency_data = {
            "ResNet50": {"P50": 22.5, "P95": 25.3, "P99": 28.1},
            "YOLO V8": {"P50": 30.2, "P95": 35.8, "P99": 42.5},
            "Segformer": {"P50": 18.5, "P95": 20.3, "P99": 23.8},
            "EfficientDet": {"P50": 15.2, "P95": 17.5, "P99": 19.8},
        }

    x = np.arange(len(workloads))
    width = 0.25

    p50 = [latency_data[w]["P50"] for w in workloads]
    p95 = [latency_data[w]["P95"] for w in workloads]
    p99 = [latency_data[w]["P99"] for w in workloads]

    ax.bar(x - width, p50, width, label="P50 (Median)", alpha=0.8)
    ax.bar(x, p95, width, label="P95", alpha=0.8)
    ax.bar(x + width, p99, width, label="P99", alpha=0.8)

    ax.set_xlabel("Workload", fontsize=12, fontweight="bold")
    ax.set_ylabel("Latency (ms)", fontsize=12, fontweight="bold")
    ax.set_title("Latency Percentiles Comparison", fontsize=14, fontweight="bold")
    ax.set_xticks(x)
    ax.set_xticklabels(workloads, rotation=45, ha="right")
    ax.legend()
    ax.grid(axis="y", alpha=0.3)

    plt.tight_layout()
    return fig


def plot_latency_throughput_tradeoff():
    """Create latency vs throughput trade-off."""
    fig, ax1 = plt.subplots(figsize=(12, 6))

    global INPUT
    if INPUT and "tradeoff" in INPUT:
        batch_sizes, latencies, throughputs = INPUT["tradeoff"]
    else:
        batch_sizes = [1, 2, 4, 8, 16, 32, 64]
        latencies = [15.0, 12.5, 10.8, 9.5, 8.8, 8.5, 8.3]
        throughputs = [66.7, 160.0, 370.4, 842.1, 1136.4, 1882.4, 2409.6]

    color = "tab:blue"
    ax1.set_xlabel("Batch Size", fontsize=12, fontweight="bold")
    ax1.set_ylabel("Latency (ms)", color=color, fontsize=12, fontweight="bold")
    ax1.plot(batch_sizes, latencies, color=color, marker="o", linewidth=2.5, markersize=8)
    ax1.tick_params(axis="y", labelcolor=color)
    ax1.grid(alpha=0.3)

    ax2 = ax1.twinx()
    color = "tab:orange"
    ax2.set_ylabel("Throughput (items/sec)", color=color, fontsize=12, fontweight="bold")
    ax2.plot(batch_sizes, throughputs, color=color, marker="s", linewidth=2.5, markersize=8)
    ax2.tick_params(axis="y", labelcolor=color)

    fig.suptitle("Latency vs Throughput Trade-off Analysis", fontsize=14, fontweight="bold")

    plt.tight_layout()
    return fig


def plot_power_vs_performance():
    """Create power vs performance scatter plot."""
    fig, ax = plt.subplots(figsize=(10, 6))

    global INPUT
    if INPUT and "power_perf" in INPUT:
        workloads, power_values, performance_values = INPUT["power_perf"]
    else:
        workloads = ["ResNet50", "YOLO V8", "Segformer", "EfficientDet", "MobileNet"]
        power_values = [65.2, 82.5, 55.3, 48.1, 35.6]
        performance_values = [44.4, 33.1, 54.1, 65.8, 81.2]

    colors = plt.cm.Set3(np.linspace(0, 1, len(workloads)))
    ax.scatter(power_values, performance_values, s=300, alpha=0.6, c=colors)

    for i, workload in enumerate(workloads):
        ax.annotate(
            workload, (power_values[i], performance_values[i]), xytext=(5, 5), textcoords="offset points", fontsize=10
        )

    ax.set_xlabel("Power Consumption (W)", fontsize=12, fontweight="bold")
    ax.set_ylabel("Performance (FPS)", fontsize=12, fontweight="bold")
    ax.set_title("Power vs Performance Correlation", fontsize=14, fontweight="bold")
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    return fig


def plot_gpu_memory_timeline():
    """Create GPU memory usage timeline."""
    fig, ax = plt.subplots(figsize=(14, 6))

    timestamps = np.arange(0, 60, 1)
    memory_used = 2500 + 150 * np.sin(timestamps / 20) + np.random.normal(0, 50, len(timestamps))
    memory_total = [8000] * len(timestamps)

    ax.fill_between(timestamps, 0, memory_used, alpha=0.4)
    ax.plot(timestamps, memory_used, color="blue", linewidth=2, marker="o", markersize=4, label="Memory Used (MB)")
    ax.plot(timestamps, memory_total, color="red", linewidth=2, linestyle="--", label="Total Memory (8000MB)")

    ax.set_xlabel("Time (seconds)", fontsize=12, fontweight="bold")
    ax.set_ylabel("Memory (MB)", fontsize=12, fontweight="bold")
    ax.set_title("GPU Memory Usage During LLM Inference (60s)", fontsize=14, fontweight="bold")
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    return fig


def plot_relative_performance():
    """Create relative performance comparison."""
    fig, ax = plt.subplots(figsize=(12, 6))

    global INPUT
    configs = ["Baseline", "Optimized V1", "Optimized V2", "Quantized"]
    metrics = ["latency", "throughput", "power"]

    if INPUT and "relative" in INPUT:
        data = INPUT["relative"]
        configs = list(data.keys())
    else:
        data = {
            "Baseline": {"latency": 25.5, "throughput": 100, "power": 65},
            "Optimized V1": {"latency": 22.3, "throughput": 115, "power": 62},
            "Optimized V2": {"latency": 20.8, "throughput": 130, "power": 60},
            "Quantized": {"latency": 18.5, "throughput": 145, "power": 55},
        }

    baseline = data["Baseline"]
    x = np.arange(len(configs))
    width = 0.25

    for i, metric in enumerate(metrics):
        baseline_val = baseline[metric]
        relative_vals = [data[cfg][metric] / baseline_val * 100 for cfg in configs]
        ax.bar(x + (i - 1) * width, relative_vals, width, label=metric, alpha=0.8)

    ax.axhline(y=100, color="red", linestyle="--", linewidth=2, label="Baseline")
    ax.set_ylabel("Relative Performance (%)", fontsize=12, fontweight="bold")
    ax.set_title("Optimization Impact: Relative to Baseline", fontsize=14, fontweight="bold")
    ax.set_xticks(x)
    ax.set_xticklabels(configs, rotation=45, ha="right")
    ax.legend()
    ax.grid(axis="y", alpha=0.3)

    plt.tight_layout()
    return fig


def plot_distribution_comparison():
    """Create distribution comparison."""
    fig, ax = plt.subplots(figsize=(12, 6))

    global INPUT
    np.random.seed(42)
    if INPUT and "rows" in INPUT:
        # Create small synthetic distributions around provided latencies
        baseline_vals = [
            float(r["latency"]) for r in INPUT["rows"] if r.get("config", "").lower().startswith("baseline")
        ]
        if baseline_vals:
            baseline_data = np.random.normal(baseline_vals[0], 3, 1000)
        else:
            baseline_data = np.random.normal(25, 3, 1000)

        opt_vals = [float(r["latency"]) for r in INPUT["rows"] if "optimized" in r.get("config", "").lower()]
        if opt_vals:
            optimized_data = np.random.normal(opt_vals[0], 2.5, 1000)
        else:
            optimized_data = np.random.normal(22, 2.5, 1000)

        quant_vals = [float(r["latency"]) for r in INPUT["rows"] if "quant" in r.get("config", "").lower()]
        if quant_vals:
            quantized_data = np.random.normal(quant_vals[0], 2, 1000)
        else:
            quantized_data = np.random.normal(20, 2, 1000)
    else:
        baseline_data = np.random.normal(25, 3, 1000)
        optimized_data = np.random.normal(22, 2.5, 1000)
        quantized_data = np.random.normal(20, 2, 1000)

    ax.hist(baseline_data, bins=40, alpha=0.6, label="Baseline")
    ax.hist(optimized_data, bins=40, alpha=0.6, label="Optimized")
    ax.hist(quantized_data, bins=40, alpha=0.6, label="Quantized")

    ax.set_xlabel("Latency (ms)", fontsize=12, fontweight="bold")
    ax.set_ylabel("Frequency", fontsize=12, fontweight="bold")
    ax.set_title("Latency Distribution Comparison", fontsize=14, fontweight="bold")
    ax.legend()
    ax.grid(axis="y", alpha=0.3)

    plt.tight_layout()
    return fig


def generate_report(output_path="performance_report.pdf", input_path=None):
    """Generate complete PDF report."""
    print("=" * 80)
    print("PERFORMANCE ANALYSIS PDF REPORT GENERATOR")
    print("=" * 80)

    figures = []
    captions = [
        "Latency Percentiles Across Workloads",
        "Batch Size Trade-off: Latency vs Throughput",
        "Power Efficiency: Workload Comparison",
        "GPU Memory Usage During LLM Inference",
        "Optimization Impact: Relative Performance",
        "Latency Distribution Analysis",
    ]

    # Load input data if provided
    global INPUT
    if input_path:
        loaded = load_input_data(input_path)
        if loaded:
            INPUT = loaded
        else:
            print(f"Warning: failed to load input file {input_path}; using synthetic data")

    print("\nüìä Creating graphs...")
    print("1Ô∏è‚É£  Latency percentiles...")
    figures.append(plot_latency_percentiles())

    print("2Ô∏è‚É£  Latency vs throughput trade-off...")
    figures.append(plot_latency_throughput_tradeoff())

    print("3Ô∏è‚É£  Power vs performance...")
    figures.append(plot_power_vs_performance())

    print("4Ô∏è‚É£  GPU memory timeline...")
    figures.append(plot_gpu_memory_timeline())

    print("5Ô∏è‚É£  Relative performance comparison...")
    figures.append(plot_relative_performance())

    print("6Ô∏è‚É£  Distribution comparison...")
    figures.append(plot_distribution_comparison())

    print(f"\nüìÑ Generating PDF: {output_path}")

    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", exist_ok=True)

    with PdfPages(output_path) as pdf:
        # Title page
        title_fig = create_title_page()
        pdf.savefig(title_fig, bbox_inches="tight")
        plt.close(title_fig)

        # Content pages
        for fig, caption in zip(figures, captions):
            fig.text(0.5, 0.02, caption, ha="center", fontsize=10, style="italic", color="gray")
            pdf.savefig(fig, bbox_inches="tight")
            plt.close(fig)

        # PDF metadata
        d = pdf.infodict()
        d["Title"] = "Performance Analysis Report"
        d["Author"] = "AutoPerfPy"
        d["Subject"] = "Performance Analysis Report"
        d["Keywords"] = "Performance, Analysis, Benchmarking"
        d["CreationDate"] = datetime.now()

    print(f"‚úÖ Report generated successfully!")
    print("\n" + "=" * 80)
    print(f"Output: {output_path}")
    print(f"Size: {os.path.getsize(output_path) / 1024:.1f} KB")
    print(f"Graphs: {len(figures)}")
    print("=" * 80 + "\n")

    return output_path


def main():
    parser = argparse.ArgumentParser(description="Generate consolidated performance analysis PDF report")
    parser.add_argument(
        "--output",
        type=str,
        default="performance_report.pdf",
        help="Output PDF filename (default: performance_report.pdf)",
    )
    parser.add_argument("--input", type=str, default=None, help="Optional input CSV with benchmark data")

    args = parser.parse_args()

    generate_report(args.output, args.input)


if __name__ == "__main__":
    main()
